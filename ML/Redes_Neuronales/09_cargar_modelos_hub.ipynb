{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Modelos pre entrenados de Tensorflow Hub__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustar los datos en funcion del modelo a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con las imagenes de que tenemos precargadas en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 10:32:00.499368: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 10:32:01.008793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-13 10:32:01.008890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-13 10:32:01.173098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 10:32:01.582015: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 10:32:01.591037: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-13 10:32:04.649572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# librerias\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# libreria para trabajar con las bases de datos \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuevos data generators, donde modificamos el target size y el color mode. Esto será aplicado a los 3 generadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27455 images belonging to 24 classes.\n",
      "Found 1425 images belonging to 24 classes.\n",
      "Found 7172 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "# ruta de los archivos\n",
    "train_dir = './sign-language-img/Train'\n",
    "test_dir = './sign-language-img/Test'\n",
    "\n",
    "# data generator\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255, validation_split=0.2)\n",
    "\n",
    "# generator para test, training y validation adaptados al modelo\n",
    "train_generator_resize = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 128,\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'rgb',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "# para datos de validacion\n",
    "validation_generator_resize = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 128,\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'rgb',\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "# para datos de prueba\n",
    "test_generator_resize = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 128,\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las clases de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clases\n",
    "classes = [char for char in string.ascii_uppercase if char != 'J' if char != 'Z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar modelo de Hub a utilizar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la libreria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de todo, debemos detectar qué configuración usaremos, para esta ocasión usaremos la arquitectura [MobileNetV1](https://www.kaggle.com/models/google/mobilenet-v1/tensorFlow2/050-160-classification/1?tfhub-redirect=true).\n",
    "\n",
    "Este modelo es secuencial, por lo que stackearemos sus capas en un modelo de este tipo.\n",
    "\n",
    "Para los modelos del hub, basta con agregar una capa de entrada, añadir la capa de KerasLayer con la URL del sitio (no olvides configurarlo como no entrenable) y a partir de este punto podrás inyectar tu propia arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/4'\n",
    "\n",
    "model_hub = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(150, 150, 3)),\n",
    "    hub.KerasLayer(model_url, trainable=False),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(len(classes), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de compilar la red debemos hacer build al modelo, siendo explícitos en las dimensiones de su tensor, en este caso MobileNet soporta frames de video, por lo que para usarlo como detector de imágenes bastará con anular esta dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1001)              1343049   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1001)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               128256    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1474401 (5.62 MB)\n",
      "Trainable params: 131352 (513.09 KB)\n",
      "Non-trainable params: 1343049 (5.12 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hub.build((None,150,150,3))\n",
    "model_hub.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar en la arquitectura que se añade toda la capa cargada y posteriormente nuestra arquitectura.\n",
    "\n",
    "El proceso de compilación y entrenamiento será el tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hub.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 10:36:50.113740: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34560000 exceeds 10% of free system memory.\n",
      "2024-11-13 10:36:58.672016: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 34560000 exceeds 10% of free system memory.\n",
      "2024-11-13 10:36:59.412676: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 34572288 bytes after encountering the first element of size 34572288 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n",
      "2024-11-13 10:36:59.429208: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080000 exceeds 10% of free system memory.\n",
      "2024-11-13 10:37:00.377389: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080000 exceeds 10% of free system memory.\n",
      "2024-11-13 10:37:00.747956: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 267s 1s/step - loss: 1.5760 - accuracy: 0.5150 - val_loss: 0.8558 - val_accuracy: 0.6989\n",
      "Epoch 2/2\n",
      "215/215 [==============================] - 235s 1s/step - loss: 0.6649 - accuracy: 0.7804 - val_loss: 0.6025 - val_accuracy: 0.7832\n"
     ]
    }
   ],
   "source": [
    "history_hub = model_hub.fit(\n",
    "    train_generator_resize,\n",
    "    epochs=2,\n",
    "    validation_data=validation_generator_resize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de modelos son útiles a la hora de procesar imágenes en vivo (como en cámaras de drones). Pero traen en consecuencias una pérdida de precisión, dependerá de tu proyecto aceptar estas alternativas o sacrificar tiempo de cómputo por mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluemos el modelo cn los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 62s 1s/step - loss: 0.6063 - accuracy: 0.7885\n"
     ]
    }
   ],
   "source": [
    "results = model_hub.evaluate(test_generator_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6063175797462463, 0.7884829640388489]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tenemos una precision muy baja respecto al modelo anterior y una perdida alta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
