{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dab1c5",
   "metadata": {},
   "source": [
    "# __Regularizacion en Modelos Lineales__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c376f73",
   "metadata": {},
   "source": [
    "La mejor manera de reducir el sobreajuste en un modelo lineal es con la regularizacion. Este proceso consiste en restringir los pesos del modelo. Existen tres formas de restringir los pesos como veremos a continuacion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594f4f7",
   "metadata": {},
   "source": [
    "## Regresion Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfa1c3",
   "metadata": {},
   "source": [
    "Es una version regularizada de la regresion lineal y consiste en agregar el termino $\\alpha \\sum_{i}^{n} \\theta_{i}^2$ a la funcion de costo. Esto obliga al algoritmo de aprendizaje a no solo ajustar los datos, sino tambien a mantener los pesos del modelo los mas pequeno posible.\n",
    "\n",
    "- Este termino solo debe agregarse a la funcion de costo durante el entrenamiento. Una vez entrenado, se evalua el rendimiento del modelo utilizando una medida no regularizada.\n",
    "\n",
    "- Es comun que la funcion de costo usada durante el entrenamiento sea diferente de la medida de rendimiento del modelo.\n",
    "\n",
    "- La funcion de costo utilizada en el entrenamiento debe tener derivadas amigables para la optimizacion mientras que la medida de rendimiento debe estar lo mas cerca posible del objetivo final.\n",
    "\n",
    "- El parametro $\\alpha$ controla cuanto desea regularizar el modelo. Si $\\alpha=0$, la regresion Ridge se vuelve una regresion lineal. Si $\\alpha$ es muy grande, todos los pesos terminan muy cerca de cero y el resultado es una linea plana que pasa por la media de los datos.\n",
    "\n",
    "- Es importante escalar los datos con _StandarScaler_ antes de alicar la regularizacion ya que es sensible a la escala de las caracteristicas de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3bf4b",
   "metadata": {},
   "source": [
    "## Como funciona?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00975d6f",
   "metadata": {},
   "source": [
    "Vamos a suponer un problema de regresion lineal simple. Queremos predecir $y$ a partir de $x$ usando la funcion lineal:\n",
    "\n",
    "$$y_i = wx_i + b$$\n",
    "\n",
    "Los datos con los que contamos son los siguientes:\n",
    "\n",
    "| x | y   |\n",
    "| - | --- |\n",
    "| 1 | 2   |\n",
    "| 2 | 4   |\n",
    "| 3 | 6.1 |\n",
    "\n",
    "\n",
    "> Notese que hay una relacion lineal entre los datos de $y \\approx 2x$ con un poco de ruido.\n",
    "\n",
    "En regresion lineal, se busca minimizar el error cuadratico medio cuya formula viene dada por:\n",
    "\n",
    "$$ J(w,b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (wx_i + b))^2$$\n",
    "\n",
    "Esta funcion busca ajustar el modelo lo mejor posible a los datos. En nuestro caso, y para simplificar el calculo, el intercepto es cero ($b=0$) por lo que el error cuadratico medio nos quedaria:\n",
    "\n",
    "$$ J(w) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - wx_i)^2$$\n",
    "\n",
    "Los pesos $w$ son los encargados de ajustar el modelo a los datos. Cuando se presenta sobreajuste, una forma de contrarrestarlo es modificando los pesos y de esa forma reajustar el modelo a los datos. Para obtener la formula de los pesos $w$ procedemos como sigue:\n",
    "\n",
    "- Derivamos $J(w)$ respecto a $w$ y lo igualamos a cero:\n",
    "\n",
    "$$\\frac{\\mathrm{d} J}{\\mathrm{d} w} = \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - wx_i)(-x_i) = 0$$\n",
    "\n",
    "- El 2 deja de tener relevancia, por tanto\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i(y_i - wx_i) = 0$$\n",
    "\n",
    "- Expandimos\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n}x_{i}^2 = 0$$\n",
    "\n",
    "- Despejamos $w$\n",
    "\n",
    "$$w = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_{i}^2}$$\n",
    "\n",
    "La formula del error cuadratico medio, al agregar la regularizacion Ridge, queda de la siguiente forma:\n",
    "\n",
    "$$J_{ridge}(w) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - wx_i)^2 + \\alpha w^2$$\n",
    "\n",
    "Aplicando el mismo procedimiento anterior para obtener $w_{ridge}$ quedaria:\n",
    "\n",
    "$$w_{ridge} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_{i}^2 + \\alpha}$$\n",
    "\n",
    "A partir de esta formula podemos deducir que:\n",
    "\n",
    "- Si $\\alpha$ = 0, llegamos a la regresion lineal estandar.\n",
    "\n",
    "- Si $0 > \\alpha > 1$, regula los pesos del modelo manteniendolos pequenos lo que reduce el riesgo a sobreajuste.\n",
    "\n",
    "- Si $\\alpha >> 0$, los pesos terminan muy cerca de cero.\n",
    "\n",
    "Ahora, como en nuestro ejemplo solo hay un peso, vamos a calcularlo con y sin regularizacion ($w$ y $w_{ridge}$)\n",
    "\n",
    "$$w = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_{i}^2} = \\frac{1\\cdot 2 + 2 \\cdot 4 + 3\\cdot 6.1}{1^2 + 2^2 + 3^2} = \\frac{2 + 8 + 18.3}{1 + 4 + 9} = \\frac{28.3}{14} \\approx 2.021$$\n",
    "\n",
    "El peso estimado $w$ es cercano a 2.\n",
    "\n",
    "Para calcular $w_{ridge}$ utilizamos $\\alpha = 1$\n",
    "\n",
    "$$w_{ridge} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_{i}^2 + \\alpha} = \\frac{1\\cdot 2 + 2 \\cdot 4 + 3\\cdot 6.1}{1^2 + 2^2 + 3^2 + 1} = \\frac{2 + 8 + 18.3}{1 + 4 + 9 + 1} = \\frac{28.3}{15} \\approx 1.887$$\n",
    "\n",
    "El peso estimado utilizando rigde $w_{ridge}$ se reduce ligeramente, lo que evidencia el efecto de la penalizacion.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "> rigde no elimina las variables, solo reduce los pesos.\n",
    "\n",
    "> Cuanto mayor sea $\\alpha$, mas se reducen los pesos.\n",
    "\n",
    "> En modelos con muchas variables correlacionadas, ayuda a distribuir mejor el peso y evitar sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb2e77",
   "metadata": {},
   "source": [
    "## Cuando usar Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab6952",
   "metadata": {},
   "source": [
    "- Cuando hay muchas variables numericas.\n",
    "\n",
    "- Cuando hay colinealidad entre variables.\n",
    "\n",
    "- Cuando el modelo presenta sobreajuste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
