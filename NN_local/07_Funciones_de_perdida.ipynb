{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "154d2858",
      "metadata": {
        "id": "154d2858"
      },
      "source": [
        "# **Funciones de Pérdida en Redes Neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qTCwGJadDK9x",
      "metadata": {
        "id": "qTCwGJadDK9x"
      },
      "source": [
        "## **¿Qué es una función de pérdida?**\n",
        "\n",
        "Es una métrica matemática que mide el nivel de error o discrepancia entre las predicciones de un modelo de aprendizaje automático y los valores reales que se espera que el modelo prediga"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neqjFlMHD2w5",
      "metadata": {
        "id": "neqjFlMHD2w5"
      },
      "source": [
        "## **Tipos Comunes de Funciones de Pérdida**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f724722",
      "metadata": {},
      "source": [
        "### **Para Problemas de Regresión**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4939fd3",
      "metadata": {},
      "source": [
        "#### 1. Error Cuadrático Medio (MSE)\n",
        "\n",
        "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "Keras: `mean_squared_error`\n",
        "\n",
        "\n",
        "#### 2. Error Absoluto Medio (MAE)\n",
        "\n",
        "$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
        "\n",
        "Keras: `mean_absolute_error`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a7095f",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f23b4efc",
      "metadata": {},
      "source": [
        "### **Para Problemas de Clasificación**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khPFmiMfF6qv",
      "metadata": {
        "id": "khPFmiMfF6qv"
      },
      "source": [
        "\n",
        "#### 1. Entropía Cruzada Binaria (Binary Crossentropy)\n",
        "\n",
        "$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
        "\n",
        "Donde;\n",
        "\n",
        "$n$: número de muestras.\n",
        "\n",
        "$y_i$: valor real de la clase para la muestra $i$ $(0\\ o\\ 1)$.\n",
        "\n",
        "$\\hat{y}_i$: probabilidad predicha por el modelo para la clase positiva $(1)$, es decir, $\\hat{y}_i ∈[0,1] $\n",
        "\n",
        "$\\text{log}$: el logaritmo natural.\n",
        "\n",
        "*- Se usado en problemas de clasificación binaria*\n",
        "\n",
        "Keras: `binary_crossentropy`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 2.  Entropía Cruzada Categórica (Categorical Crossentropy)\n",
        "\n",
        "$\\text{CCE} = -\\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{ic} \\log(\\hat{y}_{ic})$\n",
        "\n",
        "*- Se usado en problemas de clasificación multiclase*\n",
        "\n",
        "Donde:\n",
        "\n",
        "$n$: es el número de muestras.\n",
        "\n",
        "$C$: es el número de clases.\n",
        "\n",
        "$y_{ic}$: es el valor real ($1$ si la muestra $i$ pertenece a la clase $c$, de lo contrario $0$).\n",
        "\n",
        "$\\hat{y}_{ic}$: es la probabilidad predicha para que la muestra $i$ pertenezca a la clase $c$.\n",
        "\n",
        "Keras: `categorical_crossentropy`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 3.  Sparse Categorical Crossentropy\n",
        "\n",
        "$\\text{Sparse CCE} = -\\sum_{i=1}^{n} \\log(\\hat{y}_{i, y_i})$\n",
        "\n",
        "*- Se usado en problemas de clasificación multiclase. Se utiliza cuando las etiquetas de clase están en formato entero en lugar de formato one-hot*\n",
        "\n",
        "Donde:\n",
        "\n",
        "$n$: es el número de muestras.\n",
        "\n",
        "$y_i$: es la clase verdadera de la muestra $i$, expresada como un índice entero en el rango $[0 , C−1]$, donde $C$ es el número de clases.\n",
        "\n",
        "$\\hat{y}_{i, y_i}$: es la probabilidad predicha por el modelo para la clase verdadera $y_i$ de la muestra $i$.\n",
        "\n",
        "Keras: `sparse_categorical_crossentropy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k7PjCr2PD-9",
      "metadata": {
        "id": "_k7PjCr2PD-9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "t1wFSQOAYlsu",
      "metadata": {
        "id": "t1wFSQOAYlsu"
      },
      "source": [
        "## **Ejemplo desde la intuición**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lbbWz7W-Y1T-",
      "metadata": {
        "id": "lbbWz7W-Y1T-"
      },
      "source": [
        "Supón que para una muestra el valor real es $y_i = 1$ (clase positiva), y el modelo predice $\\hat{y}_i = 0.9$ (una probabilidad alta de ser 1). Entonces, la parte de la fórmula relevante sería:\n",
        "\n",
        "$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FlskfbLkwyNi",
      "metadata": {
        "id": "FlskfbLkwyNi"
      },
      "source": [
        "\n",
        "$\\text{BCE} = 1. \\log(0.9) + (1 - 1) \\log(1 - 0.9)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dPxUxxdw0nl",
      "metadata": {
        "id": "3dPxUxxdw0nl"
      },
      "source": [
        "\n",
        "$y_i. \\text{log}(\\hat{y}_i)\\ =\\ 1\\ \\text{x}\\ \\text{log}(0.9)\\ =\\ \\text{log}(0.9)\\ \\thickapprox\\ -0.105$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24n64p1dw30R",
      "metadata": {
        "id": "24n64p1dw30R"
      },
      "source": [
        "**Interpretación**: *Este valor negativo pequeño indica que el modelo hizo una buena predicción*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55LEjft0ySon",
      "metadata": {
        "id": "55LEjft0ySon"
      },
      "source": [
        "Si en cambio el modelo predice $\\hat{y}_i = 0.1$ (una probabilidad baja de ser 1), entonces:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GpKomHNPyp9P",
      "metadata": {
        "id": "GpKomHNPyp9P"
      },
      "source": [
        "$\\text{BCE} = 1. \\log(0.1) + (1 - 1) \\log(1 - 0.1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yrlp3JhTyhmR",
      "metadata": {
        "id": "Yrlp3JhTyhmR"
      },
      "source": [
        "$y_i. \\text{log}(\\hat{y}_i)\\ =\\ 1\\ . \\text{log}(0.1)\\ =\\ \\text{log}(0.1)\\ \\thickapprox\\ -2.3$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ska-1MflwxSS",
      "metadata": {
        "id": "ska-1MflwxSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "E4i4ji1mzDQD",
      "metadata": {
        "id": "E4i4ji1mzDQD"
      },
      "source": [
        "**Para el caso en que la etiqueta de la muestra evaluada sea `clase 0`**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LOG7I9yNvL_5",
      "metadata": {
        "id": "LOG7I9yNvL_5"
      },
      "source": [
        "Sea:\n",
        "\n",
        "\n",
        "$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
        "\n",
        "Para este caso $y_i$ = 0\n",
        "\n",
        "Y el modelo predice $\\hat{y}_i = 0.1$ (una probabilidad baja de ser 1), entonces:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xpm6K4jFxfD_",
      "metadata": {
        "id": "xpm6K4jFxfD_"
      },
      "source": [
        "$\\text{BCE} = 0. \\log(0.1) + (1 - 0) \\log(1 - 0.1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j0KM-RgBxq96",
      "metadata": {
        "id": "j0KM-RgBxq96"
      },
      "source": [
        "$(1 - y_i). \\text{log}(\\hat{y}_i)\\ =\\ (1 - 0)\\ . \\text{log}(0.9)\\ =\\ \\text{log}(0.9)\\ \\thickapprox\\ -0.105$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ea_8WPDQzikn",
      "metadata": {
        "id": "Ea_8WPDQzikn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "z7DNGSREPLSB",
      "metadata": {
        "id": "z7DNGSREPLSB"
      },
      "source": [
        "## **Ejemplos Prácticos:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KOiEPkikPFYn",
      "metadata": {
        "id": "KOiEPkikPFYn"
      },
      "source": [
        "#### Ejemplo 1: Función de Pérdida de Regresión (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ERCst5KIPDyP",
      "metadata": {
        "id": "ERCst5KIPDyP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Generar datos simulados\n",
        "X = np.random.rand(100, 1)\n",
        "y = 3 * X + 2 + np.random.randn(100, 1) * 0.1  # Relación lineal con algo de ruido\n",
        "\n",
        "# Definir un modelo simple de red neuronal\n",
        "model = Sequential([\n",
        "    Dense(1, input_dim=1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo con la función de pérdida MSE\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Graficar la pérdida durante el entrenamiento\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Pérdida (MSE) durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida (MSE)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jC8VNz_ZQnvY",
      "metadata": {
        "id": "jC8VNz_ZQnvY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "neZNGpJoQoOF",
      "metadata": {
        "id": "neZNGpJoQoOF"
      },
      "source": [
        "#### Ejemplo 2: Función de Pérdida para Clasificación (Entropía Cruzada Binaria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rgpREZdLPZbj",
      "metadata": {
        "id": "rgpREZdLPZbj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generar datos simulados de clasificación binaria\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Crear un modelo simple para clasificación\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=20),\n",
        "    Dense(1, activation='sigmoid')  # Salida binaria\n",
        "])\n",
        "\n",
        "# Compilar el modelo con Binary Crossentropy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
        "\n",
        "# Graficar la pérdida y la precisión durante el entrenamiento\n",
        "plt.plot(history.history['loss'], label='Pérdida Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida Validación')\n",
        "plt.title('Pérdida (Binary Crossentropy)')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Precisión Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisión Validación')\n",
        "plt.title('Precisión del modelo')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Precisión')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HOeOn5VSPZLW",
      "metadata": {
        "id": "HOeOn5VSPZLW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
